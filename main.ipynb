{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a DQN Network following along [this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# In interactive mode (enabled with plt.ion()):\n",
    "# - newly created figures will be shown immediately;\n",
    "# - figures will automatically redraw on change;\n",
    "# - `pyplot.show` will not block by default.\n",
    "plt.ion()\n",
    "\n",
    "# check if using DirectML or not\n",
    "import imp\n",
    "try:\n",
    "    imp.find_module('torch_directml')\n",
    "    # https://learn.microsoft.com/en-us/windows/ai/directml/gpu-pytorch-windows\n",
    "    device = torch_directml.device()\n",
    "except ImportError:\n",
    "    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "A replay memory is basically a storage of all previous state transitions and their rewards.\n",
    "[resource](https://deeplizard.com/learn/video/Bcuj2fTH4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experience memory: https://deeplizard.com/learn/video/Bcuj2fTH4_4\n",
    "# At time t, the agent's experience e_t is defined as this tuple:\n",
    "# e_t = (s_t, a_t, r_{t+1}, s_{t+1})\n",
    "# which gives us information about its current state, the action taken from state s_t, the reward at\n",
    "# t+1, and the next state in the environment (at t+1). The last one, we won't know of course unless\n",
    "# this experience is in at least 1 timestep in the past. (you can't see the future!)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_sze):\n",
    "        return random.sample(self.memory, batch_sze)\n",
    "\n",
    "    def __len(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # nn.Linear(in, uot) is like the \"space\" between 2 layers. it takes an input and maps it to neurons out.\n",
    "        # resource https://www.sharetechnote.com/html/Python_PyTorch_nn_Linear_01.html\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    # x is a singular element: determine next action\n",
    "    # x is a batch (multiple el.): for optimization (diff. word for training)\n",
    "    # returns a tensor\n",
    "    def forward(self, x):\n",
    "        # basically, drive x through the whole network. Throughout this process, x may change its shape\n",
    "        # ReLU(x) = max(0, x) (basically), comparable to sigmoid\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters: constants that are set before the machine learning process begins\n",
    "\n",
    "# get size of action space\n",
    "n_actions = env.action_space.n\n",
    "# https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space\n",
    "# this will return initial state parameters like pos, vel, etc\n",
    "# will be stochastically initialized\n",
    "state, info = env.reset()\n",
    "# size of state, or, things we can define our environment from\n",
    "n_observations = len(state)\n",
    "\n",
    "# TODO understand why we initialize 2 networks\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "# load all parameters so that they're the \"same\" network\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# AdamW is laso called SGD or Stochastic Gradient Descent, you can look into it here√á\n",
    "# https://dev.to/amananandrai/10-famous-machine-learning-optimizers-1e22\n",
    "# parameters are the variables we want to change over the training. Conveniently, they all get outputted on\n",
    "# Module().parameters()\n",
    "# a layer like nn.Linear() automatically initializes these without you having to manually assign\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
