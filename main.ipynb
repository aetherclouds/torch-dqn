{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a DQN Network following along [this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kocze\\AppData\\Local\\Temp\\ipykernel_13684\\3814650790.py:30: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch_directml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     imp\u001b[39m.\u001b[39mfind_module(\u001b[39m'\u001b[39m\u001b[39mtorch_directml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# https://learn.microsoft.com/en-us/windows/ai/directml/gpu-pytorch-windows\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     device \u001b[39m=\u001b[39m torch_directml\u001b[39m.\u001b[39mdevice()\n\u001b[0;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch_directml' is not defined"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# In interactive mode (enabled with plt.ion()):\n",
    "# - newly created figures will be shown immediately;\n",
    "# - figures will automatically redraw on change;\n",
    "# - `pyplot.show` will not block by default.\n",
    "plt.ion()\n",
    "\n",
    "# check if using DirectML or not\n",
    "import imp\n",
    "try:\n",
    "    imp.find_module('torch_directml')\n",
    "    # https://learn.microsoft.com/en-us/windows/ai/directml/gpu-pytorch-windows\n",
    "    device = torch_directml.device()\n",
    "except ImportError:\n",
    "    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "A replay memory is basically a storage of all previous state transitions and their rewards.\n",
    "[resource](https://deeplizard.com/learn/video/Bcuj2fTH4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experience memory: https://deeplizard.com/learn/video/Bcuj2fTH4_4\n",
    "# At time t, the agent's experience e_t is defined as this tuple:\n",
    "# e_t = (s_t, a_t, r_{t+1}, s_{t+1})\n",
    "# which gives us information about its current state, the action taken from state s_t, the reward at\n",
    "# t+1, and the next state in the environment (at t+1). The last one, we won't know of course unless\n",
    "# this experience is in at least 1 timestep in the past. (you can't see the future!)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_sze):\n",
    "        return random.sample(self.memory, batch_sze)\n",
    "\n",
    "    def __len(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The interesting thing is that we will be outputting the Q-value for the 2 available actions (move right/left), not necessarily the probabilty. Hence, the network is trying to predict the expected return of taking that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # nn.Linear(in, uot) is like the \"space\" between 2 layers. it takes an input and maps it to neurons out.\n",
    "        # resource https://www.sharetechnote.com/html/Python_PyTorch_nn_Linear_01.html\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    # x is a singular element: determine next action\n",
    "    # x is a batch (multiple el.): for optimization (diff. word for training)\n",
    "    # returns a tensor\n",
    "    def forward(self, x):\n",
    "        # basically, drive x through the whole network. Throughout this process, x may change its shape\n",
    "        # ReLU(x) = max(0, x) (basically), comparable to sigmoid\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS: constants that are set before the machine learning process begins\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR = 1e-4\n",
    "\n",
    "# get size of action space\n",
    "n_actions = env.action_space.n\n",
    "# https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space\n",
    "# this will return initial state parameters like pos, vel, etc. will be stochastically initialized\n",
    "state, info = env.reset()\n",
    "# size of state space\n",
    "n_observations = len(state)\n",
    "\n",
    "# why do we have 2 networks? -> https://stackoverflow.com/a/59869307\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "# initialize target NN from policy NN - basically clone it -> https://pytorch.org/tutorials/beginner/saving_loading_models.html \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# AdamW is also called SGD or Stochastic Gradient Descent, you can look into it here:\n",
    "# https://dev.to/amananandrai/10-famous-machine-learning-optimizers-1e22\n",
    "# with `amsgrad=True`, we're using its variation:\n",
    "# https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html\n",
    "# parameters are the variables we want to change over the training. Conveniently, they all get outputted on\n",
    "# Module().parameters()\n",
    "# a layer like nn.Linear() automatically randomly initializes them.\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(capacity=10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
